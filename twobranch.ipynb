{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from Super_Resolution.data import DIV2K\n",
    "from Super_Resolution.model.edsr import edsr\n",
    "from Super_Resolution.train import EdsrTrainer\n",
    "from Super_Resolution.model import resolve_single\n",
    "from Super_Resolution.utils import load_image, plot_sample\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense, Conv2D, Activation, Input #changed 'keras.' to 'tensorflow.keras.'\n",
    "from keras.models import Sequential, Model #changed 'keras.' to 'tensorflow.keras.'\n",
    "from keras import optimizers #changed 'keras.' to 'tensorflow.keras.'\n",
    "import cv2\n",
    "import math\n",
    "import glob\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import array_to_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from IPython.display import display\n",
    "import sys\n",
    "from keras.models import load_model\n",
    "import mtcnn\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "import random\n",
    "from pathlib import Path\n",
    "from keras import applications\n",
    "from keras import layers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "from keras.applications import resnet\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "target_shape = (160, 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = 16\n",
    "scale = 4\n",
    "downgrade = 'bicubic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SR = edsr(scale=scale, num_res_blocks=depth)\n",
    "SR.load_weights('Super_Resolution/weights/edsr-16-x4/weights.h5')\n",
    "for layer in SR.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve(model, lr_batch):\n",
    "    lr_batch = tf.cast(lr_batch, tf.float32)\n",
    "    sr_batch = model(lr_batch)\n",
    "    sr_batch = tf.clip_by_value(sr_batch, 0, 255)\n",
    "    sr_batch = tf.round(sr_batch)\n",
    "    sr_batch = tf.cast(sr_batch, tf.uint8)\n",
    "    return sr_batch\n",
    "\n",
    "def resolve_and_plot(lr):\n",
    "    lr = tf.image.resize(lr, [36,36], method = 'bicubic')\n",
    "    sr = resolve(SR, lr)\n",
    "    sr = tf.image.resize(sr, [160,160], method = 'bicubic')\n",
    "    image = tf.image.convert_image_dtype(sr, tf.float32)\n",
    "    image = tf.image.per_image_standardization(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FECNN = keras.models.load_model('facenet_model/facenet_keras.h5')\n",
    "FECNN.load_weights('facenet_model/facenet_keras_weights.h5')\n",
    "for layer in FECNN.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = keras.models.load_model('facenet_model/facenet_keras.h5')\n",
    "temp.load_weights('facenet_model/facenet_keras_weights.h5')\n",
    "for layer in temp.layers[:-15]:\n",
    "    layer.trainable = False\n",
    "temp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "IMG_SIZE = (160,160)\n",
    "\n",
    "directory = \"Dataset/final_gen_data/train\"\n",
    "train_ds = image_dataset_from_directory(\n",
    "    directory, label_mode=None, color_mode='rgb', batch_size=BATCH_SIZE, image_size=IMG_SIZE, shuffle=True, seed=0, validation_split=None, subset=None,\n",
    "    interpolation='bicubic', follow_links=False)\n",
    "\n",
    "directory = \"Dataset/final_gen_data/val\"\n",
    "valid_ds = image_dataset_from_directory(\n",
    "    directory, label_mode=None, color_mode='rgb', batch_size=BATCH_SIZE, image_size=IMG_SIZE, shuffle=True, seed=0, validation_split=None, subset=None,\n",
    "    interpolation='bicubic', follow_links=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hr(x):\n",
    "    image = tf.image.convert_image_dtype(x, tf.float32)\n",
    "    image = tf.image.per_image_standardization(image)\n",
    "    image = FECNN(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.map(\n",
    "    lambda x: (resolve_and_plot(x), hr(x))\n",
    ")\n",
    "train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "valid_ds = valid_ds.map(\n",
    "    lambda x: (resolve_and_plot(x), hr(x))\n",
    ")\n",
    "valid_ds = valid_ds.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=(160,160,3))\n",
    "A6 = temp(input_img)\n",
    "SRFECNN = Model(input_img, A6)\n",
    "opt = optimizers.Adam(learning_rate = 0.001)\n",
    "\n",
    "SRFECNN.compile(\n",
    "    optimizer=opt,\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    "    metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "SRFECNN.summary()\n",
    "#SRFECNN.load_weights('../twobranch_extension/unfreezed15_lr=0.001.100-0.46.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_callback = ModelCheckpoint(filepath='../../twobranch_extension/unfreezed15_lr=0.001.{epoch:02d}-{val_loss:.2f}.h5',\n",
    "                                            save_weights_only = True,\n",
    "                                            period = 5,\n",
    "                                            verbose = 1,\n",
    "                                            save_best_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = SRFECNN.fit(train_ds, epochs=1, validation_data=valid_ds)#, callbacks = [model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "start = time.time()\n",
    "i=0\n",
    "FP=0\n",
    "TN=0\n",
    "FN=0\n",
    "accuracy = 0\n",
    "t=7.5\n",
    "b=0\n",
    "x = []\n",
    "from keras_vggface.vggface import VGGFace\n",
    "FECNN = VGGFace()\n",
    "def resolve_single(model, lr):\n",
    "    return resolve(model, tf.expand_dims(lr, axis=0))[0]\n",
    "#SRFECNN.load_weights('../../twobranch_extension/unfreezed15_lr=0.001.100-0.46.h5')\n",
    "for filename in os.listdir('../../SRtrain/train'):\n",
    "    img = cv2.imread(os.path.join('../../SRtrain/train',filename))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    #img = cv2.resize(img, (36,36), interpolation=cv2.INTER_CUBIC)\n",
    "    #img = resolve_single(SR,img)\n",
    "    #img = np.array(img)\n",
    "    img = cv2.resize(img, (224,224), interpolation=cv2.INTER_CUBIC)\n",
    "    img = img.astype('float32')\n",
    "    mean, std = img.mean(), img.std()\n",
    "    img = (img - mean) / std\n",
    "    img = np.array([img])\n",
    "    img = tf.convert_to_tensor(img)\n",
    "    vec = FECNN.predict(img)\n",
    "    #vec = vec/np.linalg.norm(vec)5\n",
    "    #print(vec)\n",
    "    #break\n",
    "    x.append(vec.flatten())\n",
    "    #vec1 = vec1/np.linalg.norm(vec1)\n",
    "    #a = []\n",
    "    '''for j in range(50):\n",
    "        k = 's' + str(j)\n",
    "        vec2 = dic[k]/np.linalg.norm(dic[k])\n",
    "        dist = tf.reduce_sum(K.sqrt(K.square(vec2-vec1)))\n",
    "        a.append(dist)'''  \n",
    "#print(x)\n",
    "x = StandardScaler().fit_transform(x)\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(x)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2'])\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "#plt.ylim([-30,30])\n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('FECNN Features PCA', fontsize = 20)\n",
    "colors = ['r', 'g', 'b','c','m','y','k','gold','grey','purple']\n",
    "k=[0,2,6,8,9]\n",
    "x = 10\n",
    "for i in range(10):\n",
    "    ax.scatter(principalDf.loc[range((i*x),((i*x)+x)), 'principal component 1']\n",
    "               , principalDf.loc[range((i*x),((i*x)+x)), 'principal component 2']\n",
    "               , c = colors[i]\n",
    "               , s = 50)\n",
    "#ax.legend(targets)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principalDf = pd.DataFrame(data = test2d\n",
    "             , columns = ['principal component 1', 'principal component 2'])\n",
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "#plt.ylim([-30,30])\n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('FECNN Features PCA', fontsize = 20)\n",
    "colors = ['r', 'g', 'b','c','m','y','k','gold','grey','purple']\n",
    "k=[0,2,6,8,9]\n",
    "x = 5\n",
    "for i in range(10):\n",
    "    ax.scatter(principalDf.loc[range((i*x),((i*x)+x)), 'principal component 1']\n",
    "               , principalDf.loc[range((i*x),((i*x)+x)), 'principal component 2']\n",
    "               , c = colors[i]\n",
    "               , s = 50)\n",
    "#ax.legend(targets)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THRESHOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmean = KMeans(n_clusters=2)\n",
    "labels = kmean.fit_predict(b)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_1=[]\n",
    "labels_0=[]\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    if(labels[i]==1):\n",
    "        labels_1.append(b[i])\n",
    "    else:\n",
    "        labels_0.append(b[i])\n",
    "print(len(labels_1))\n",
    "print(len(labels_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmean.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.ylim(0,1)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Image number', fontsize = 15)\n",
    "ax.set_ylabel('Distance', fontsize = 15)\n",
    "ax.set_title('KMeans clustering', fontsize = 20)\n",
    "ax.scatter(range(0,56),labels_0,color='red')\n",
    "ax.scatter(range(56,100),labels_1,color='blue')\n",
    "ax.scatter([20,53],[[0.37287224],\n",
    "       [0.11982424]],marker='x',linewidths=3,color='purple')\n",
    "ax.plot([0,66],[0.235,0.235],color='green')\n",
    "ax.legend(['Threshold','Img not in DB','Img in DB','Cluster centroids'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = b[:40]\n",
    "mini = min(c)\n",
    "#print(mini)\n",
    "d = b[40:]\n",
    "maxi = max(d)\n",
    "#print(c) maximum = 0.23314758\n",
    "print(d) #minimum = 0.23771875,0.25001103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "j=1\n",
    "temp = []\n",
    "data = np.zeros((100,512))\n",
    "train_labels = np.zeros((100,))\n",
    "for filename in os.listdir('../SRtrain/train'):\n",
    "    #print(filename)\n",
    "    hr = cv2.imread(os.path.join('../SRtrain/train',filename))\n",
    "    hr = cv2.cvtColor(hr, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    hr = hr.astype('float32')\n",
    "    mean, std = hr.mean(), hr.std()\n",
    "    hr = (hr - mean) / std\n",
    "    hr = np.array([hr])\n",
    "    hr = tf.convert_to_tensor(hr)\n",
    "    vec = FECNN.predict(hr)\n",
    "    vec = vec.flatten()\n",
    "    data[i] = vec\n",
    "    train_labels[i] = int(i/10)\n",
    "    i += 1\n",
    "    \n",
    "print(data)\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STANDARDIZATION\n",
    "\n",
    "mean_arr = []\n",
    "std_arr = []\n",
    "for i in range(len(data[0])):\n",
    "    x = data[:,i]\n",
    "    x = x.astype('float32')\n",
    "    mean, std = x.mean(), x.std()\n",
    "    x = (x - mean) / std\n",
    "    mean_arr.append(mean)\n",
    "    std_arr.append(std)\n",
    "    data[:,i] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = np.zeros((50,))\n",
    "b = []\n",
    "i=0\n",
    "test = np.zeros((50,512))\n",
    "#SRFECNN.load_weights('../../twobranch_extension/unfreezed15_lr=0.001.100-0.46.h5')\n",
    "for filename in os.listdir('../SRtrain/test'):\n",
    "    lr = cv2.imread(os.path.join('../SRtrain/test',filename))\n",
    "    lr = cv2.cvtColor(lr, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    lr = cv2.resize(lr, (16,16), interpolation=cv2.INTER_CUBIC)\n",
    "    lr = resolve_single(SR, lr)\n",
    "    lr = np.array(lr)\n",
    "    lr = cv2.resize(lr, (160,160), interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    lr = lr.astype('float32')\n",
    "    mean, std = lr.mean(), lr.std()\n",
    "    lr = (lr - mean) / std\n",
    "    \n",
    "    lr = np.array([lr])\n",
    "    lr = tf.convert_to_tensor(lr)\n",
    "    vec1 = SRFECNN.predict(lr)\n",
    "    vec1 = vec1.flatten() \n",
    "    test[i] = vec1\n",
    "    test_labels[i] = int(i/5)\n",
    "    i += 1\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STANDARDIZATION\n",
    "\n",
    "for i in range(len(test[0])):\n",
    "    x = test[:,i]\n",
    "    x = x.astype('float32')\n",
    "    x = (x - mean_arr[i]) / std_arr[i]\n",
    "    test[:,i] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model = svm.SVC(kernel = 'linear', gamma='scale', probability = True)\n",
    "svc_model.fit(data, train_labels)\n",
    "\n",
    "## Train Accuracy\n",
    "pred = svc_model.predict(data)\n",
    "train_acc = accuracy_score(train_labels, pred)\n",
    "print(\"Training Accuracy: \", train_acc)\n",
    "\n",
    "## Test Accuracy\n",
    "pred = svc_model.predict(test)\n",
    "test_acc = accuracy_score(test_labels, pred)\n",
    "print(\"Test Accuracy: \", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(data, train_labels)\n",
    "\n",
    "## Train Accuracy\n",
    "pred = model.predict(data)\n",
    "train_acc = accuracy_score(train_labels, pred)\n",
    "print(\"Training Accuracy: \", train_acc)\n",
    "\n",
    "pred = model.predict(test)\n",
    "test_acc = accuracy_score(test_labels, pred)\n",
    "print(\"Test Accuracy: \", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "#dataset = datasets.load_iris()\n",
    "\n",
    "model = SVC(kernel = 'linear',probability = True)\n",
    "model.fit(data, labels)\n",
    "print(model)\n",
    "\n",
    "#expected = dataset.target\n",
    "#predicted = model.predict(dataset.data)\n",
    "\n",
    "#print(metrics.classification_report(expected, predicted))\n",
    "#print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist1 = np.linalg.norm(data[3] - test[1])\n",
    "dist2 = np.linalg.norm(data[3] - test[10])\n",
    "print(dist1,dist2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = svc_model.predict(test)\n",
    "print(metrics.classification_report(test_labels, predicted))\n",
    "print(metrics.confusion_matrix(test_labels, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = StandardScaler().fit_transform(data)\n",
    "pca = PCA(n_components=2)\n",
    "data2d = pca.fit_transform(x)\n",
    "\n",
    "x = StandardScaler().fit_transform(test)\n",
    "pca = PCA(n_components=2)\n",
    "test2d = pca.fit_transform(x)\n",
    "#print(principalComponents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2d[:,0] *= -1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test2d[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "\n",
    "\n",
    "def make_meshgrid(x, y, h=.02):\n",
    "    \"\"\"Create a mesh of points to plot in\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: data to base x-axis meshgrid on\n",
    "    y: data to base y-axis meshgrid on\n",
    "    h: stepsize for meshgrid, optional\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xx, yy : ndarray\n",
    "    \"\"\"\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "\n",
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: matplotlib axes object\n",
    "    clf: a classifier\n",
    "    xx: meshgrid ndarray\n",
    "    yy: meshgrid ndarray\n",
    "    params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out\n",
    "\n",
    "\n",
    "# import some data to play with\n",
    "\n",
    "# Take the first two features. We could avoid this by using a two-dim dataset\n",
    "X = data2d\n",
    "y = train_labels\n",
    "\n",
    "# we create an instance of SVM and fit out data. We do not scale our\n",
    "# data since we want to plot the support vectors\n",
    "C = 1.0  # SVM regularization parameter\n",
    "models = (svm.SVC(kernel='linear', C=C),\n",
    "          svm.LinearSVC(C=C, max_iter=10000),\n",
    "          svm.SVC(kernel='rbf', gamma=0.7, C=C),\n",
    "          svm.SVC(kernel='poly', degree=3, gamma='auto', C=C))\n",
    "models = (clf.fit(X, y) for clf in models)\n",
    "\n",
    "# title for the plots\n",
    "titles = ('SVC with linear kernel',\n",
    "          'LinearSVC (linear kernel)',\n",
    "          'SVC with RBF kernel',\n",
    "          'SVC with polynomial (degree 3) kernel')\n",
    "\n",
    "# Set-up 2x2 grid for plotting.\n",
    "fig, sub = plt.subplots(2, 2)\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "X0, X1 = X[:, 0], X[:, 1]\n",
    "xx, yy = make_meshgrid(X0, X1)\n",
    "\n",
    "for clf, title, ax in zip(models, titles, sub.flatten()):\n",
    "    plot_contours(ax, clf, xx, yy,\n",
    "                  cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xlabel('Sepal length')\n",
    "    ax.set_ylabel('Sepal width')\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
